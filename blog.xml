<?xml version="1.0" encoding="UTF-8"?>
<rss  xmlns:atom="http://www.w3.org/2005/Atom" 
      xmlns:media="http://search.yahoo.com/mrss/" 
      xmlns:content="http://purl.org/rss/1.0/modules/content/" 
      xmlns:dc="http://purl.org/dc/elements/1.1/" 
      version="2.0">
<channel>
<title>Yuping Shao</title>
<link>https://yshao.github.io/mysite/blog.html</link>
<atom:link href="https://yshao.github.io/mysite/blog.xml" rel="self" type="application/rss+xml"/>
<description>Academic website of Yuping Shao</description>
<generator>quarto-1.2.269</generator>
<lastBuildDate>Wed, 18 May 2022 04:00:00 GMT</lastBuildDate>
<item>
  <title>Neuroscience/Biomedical</title>
  <link>https://yshao.github.io/mysite/blog/neuroscience/index.html</link>

  <link>https://v2-embednotion.com/DS-Blog-681b12e82e4c4973bfaad183386c27df?pvs=4</link>
  <description><![CDATA[ 



<section id="algonauts-2021" class="level1">
<h1>Algonauts 2021</h1>
<p>Due to my tangential exposure to neuroscience during my work at neuro critical care and infectious disease control. The summer after I started my PhD in Spring 2021, I was consecutively a student in the Neuromatch Academy computational neuroscience track, and then also a mentor in my home institution data science/neuroscience research experience for undergraduates (REU). I skipped the consecutive Neuromatch Academy deep learning track cause a brain can only handle so much!</p>
<p>The team I was a mentor of researched a topic on building correspondence between what the eyes see and what the brain sense, which itself is based on the Algonauts 2021 challenge. The overall ambitious goal was to link between multi-modal dataset.</p>
<p><a href="http://algonauts.csail.mit.edu/challenge.html">Website</a></p>
<p>They won the poster award at Undergraduate Research Technology Conference (URTC 2021)! It was a great experience overall. I believe at least one of the students would go into Ph.D.&nbsp;program herself.</p>
<p><a href="https://urtc.mit.edu/">Website</a></p>
</section>
<section id="neuromatch-2021" class="level1">
<h1>Neuromatch 2021</h1>
<p>For my own computational track. I based my fMRI analysis on the Human Connectome Project (HPC). The work was done in collaboration with my neuromatch cohort that include researchers from Univ Virginia - A, WashU@St. Louis, MIT, Univ in Mexico City. This is a sample preliminary concept of an application. Research questions: “can we predict participants’ accuracy on the social animacy task from between and within network connectivity? can the prediction model improve its performance when resting-state-fMRI data is included?”</p>
</section>
<section id="wpi" class="level1">
<h1>@ WPI</h1>
<section id="ultrasound-imaging-and-other-modality-imaging." class="level2">
<h2 class="anchored" data-anchor-id="ultrasound-imaging-and-other-modality-imaging.">Ultrasound Imaging and other modality imaging.</h2>
<p>Since I started at WPI at the peak of COVID in 2021. A lab specialized in photoacoustic medicine was interested in ultrasound based scan.</p>
<p>The class resulted in a imaging report using concept of co-segmention. I collaborated with fellow Ph.D.&nbsp;students from Robotics Engineering and Electrical Computer Engineering.</p>
<p><a href="https://drive.google.com/file/d/1xt-ak_X6XyyZ5AQFsjA8USIUW_u8jsz4/view">Report</a></p>
</section>
<section id="multimodal-brain-computer-interface." class="level2">
<h2 class="anchored" data-anchor-id="multimodal-brain-computer-interface.">Multimodal brain computer interface.</h2>
<p>The class is a seminar on brain computer interfaces, and included concepts from early development, clinical usage like stroke, human computer interfaces AR/VR, and other biosignals.</p>
<p>I researched in the interface to sense potential factors contribute to thresholds of good judgment. <a href="https://drive.google.com/file/d/1xt-ak_X6XyyZ5AQFsjA8USIUW_u8jsz4/view">Report</a></p>


</section>
</section>

 ]]></description>
  <category>data science</category>
  <category>misc</category>
  <guid>https://yshao.github.io/mysite/blog/neuroscience/index.html</guid>
  <pubDate>Wed, 18 May 2022 04:00:00 GMT</pubDate>
</item>
</channel>
</rss>
